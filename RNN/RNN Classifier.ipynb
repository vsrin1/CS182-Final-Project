{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import models\n",
    "import importlib\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from matplotlib import pyplot as plt\n",
    "K = tf.keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/Users/varun/School/CS182/finalproj/CS182-Spring2020-NLP-Project/models.py'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset-binary.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:int(len(df) * .8)]\n",
    "test_df = df.iloc[int(len(df) * .8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 5000, oov_token = '<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_df[\"text\"][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_thing = []\n",
    "for i in counts.keys():\n",
    "    sorted_thing.append((i, counts[i]))\n",
    "sorted_thing.sort(key = lambda x: x[1])\n",
    "sorted_thing = sorted_thing[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_thing = sorted_thing[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [np.argmax(i) + 1 for i in train_df[[\"stars_1.0\", \"stars_2.0\", \"stars_3.0\", \"stars_4.0\", \"stars_5.0\"]].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_df[\"text\"]\n",
    "stars1 = []\n",
    "stars2 = []\n",
    "stars3 = []\n",
    "stars4 = []\n",
    "stars5 = []\n",
    "for i in range(len(stars)):\n",
    "    star = stars[i]\n",
    "    if star == 1 and len(stars1) != 1000:\n",
    "        stars1.append(texts.iloc[i])\n",
    "        continue\n",
    "    if star == 2 and len(stars2) != 1000:\n",
    "        stars2.append(texts.iloc[i])\n",
    "        continue\n",
    "    if star == 3 and len(stars3) != 1000:\n",
    "        stars3.append(texts.iloc[i])\n",
    "        continue\n",
    "    if star == 4 and len(stars4) != 1000:\n",
    "        stars4.append(texts.iloc[i])\n",
    "        continue\n",
    "    if star == 5 and len(stars5) != 1000:\n",
    "        stars5.append(texts.iloc[i])\n",
    "        continue\n",
    "    if len(stars1) == 1000 and len(stars2) == 1000 and len(stars3) == 1000 and len(stars4) == 1000 and len(stars5) == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = models.tokenizer_sorted_out(stars1)\n",
    "out2 = models.tokenizer_sorted_out(stars2)\n",
    "out3 = models.tokenizer_sorted_out(stars3)\n",
    "out4 = models.tokenizer_sorted_out(stars4)\n",
    "out5 = models.tokenizer_sorted_out(stars5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_u, s2_u, cr = models.most_unique_words(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 5000, oov_token = '<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reviews, training_stars, test_reviews, test_stars = models.get_fit_samples(train_df, test_df, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 146s - loss: 1.0213 - custom_accuracy: 0.6233 - distance: 2.2286 - sum_metrics: 2.6053 - val_loss: 0.9537 - val_custom_accuracy: 0.6582 - val_distance: 2.3318 - val_sum_metrics: 2.6735\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 0.7544 - custom_accuracy: 0.7249 - distance: 1.7128 - sum_metrics: 1.9879 - val_loss: 0.6795 - val_custom_accuracy: 0.7406 - val_distance: 1.5676 - val_sum_metrics: 1.8270\n",
      "Epoch 3/5\n",
      "8000/8000 - 143s - loss: 0.6083 - custom_accuracy: 0.7720 - distance: 1.4469 - sum_metrics: 1.6749 - val_loss: 0.6711 - val_custom_accuracy: 0.7351 - val_distance: 1.5475 - val_sum_metrics: 1.8124\n",
      "Epoch 4/5\n",
      "8000/8000 - 143s - loss: 0.4983 - custom_accuracy: 0.8148 - distance: 1.2988 - sum_metrics: 1.4840 - val_loss: 0.6974 - val_custom_accuracy: 0.7450 - val_distance: 1.5774 - val_sum_metrics: 1.8324\n",
      "Epoch 5/5\n",
      "8000/8000 - 143s - loss: 0.4017 - custom_accuracy: 0.8529 - distance: nan - sum_metrics: nan - val_loss: 0.7772 - val_custom_accuracy: 0.7049 - val_distance: 1.4391 - val_sum_metrics: 1.7342\n"
     ]
    }
   ],
   "source": [
    "model, history = models.baseline_with_custom_metrics(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                     training_stars, \n",
    "                                                     test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 144s - loss: 1.1132 - custom_accuracy: 0.6438 - distance: 1.5776 - sum_metrics: 1.9338 - val_loss: 0.9195 - val_custom_accuracy: 0.7391 - val_distance: 1.4031 - val_sum_metrics: 1.6640\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 0.9158 - custom_accuracy: 0.7327 - distance: 1.3797 - sum_metrics: 1.6470 - val_loss: 0.8549 - val_custom_accuracy: 0.7619 - val_distance: 1.3387 - val_sum_metrics: 1.5768\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.8292 - custom_accuracy: 0.7607 - distance: 1.2971 - sum_metrics: 1.5364 - val_loss: 0.8441 - val_custom_accuracy: 0.7505 - val_distance: 1.3620 - val_sum_metrics: 1.6115\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.7788 - custom_accuracy: 0.7746 - distance: 1.2436 - sum_metrics: 1.4690 - val_loss: 0.8348 - val_custom_accuracy: 0.7535 - val_distance: 1.2919 - val_sum_metrics: 1.5384\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.7422 - custom_accuracy: 0.7844 - distance: nan - sum_metrics: nan - val_loss: 0.8449 - val_custom_accuracy: 0.7560 - val_distance: 1.2866 - val_sum_metrics: 1.5306\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 151s - loss: 1.1927 - custom_accuracy: 0.6428 - distance: 1.5738 - sum_metrics: 1.9311 - val_loss: 1.0001 - val_custom_accuracy: 0.7431 - val_distance: 1.3955 - val_sum_metrics: 1.6524\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 1.0130 - custom_accuracy: 0.7271 - distance: 1.3987 - sum_metrics: 1.6715 - val_loss: 1.0415 - val_custom_accuracy: 0.7247 - val_distance: 1.3549 - val_sum_metrics: 1.6302\n",
      "Epoch 3/5\n",
      "8000/8000 - 144s - loss: 0.9470 - custom_accuracy: 0.7506 - distance: 1.2941 - sum_metrics: 1.5435 - val_loss: 0.9471 - val_custom_accuracy: 0.7679 - val_distance: 1.2849 - val_sum_metrics: 1.5170\n",
      "Epoch 4/5\n",
      "8000/8000 - 143s - loss: 0.8921 - custom_accuracy: 0.7602 - distance: 1.2468 - sum_metrics: 1.4865 - val_loss: 0.9333 - val_custom_accuracy: 0.7540 - val_distance: 1.3772 - val_sum_metrics: 1.6232\n",
      "Epoch 5/5\n",
      "8000/8000 - 143s - loss: 0.8507 - custom_accuracy: 0.7726 - distance: 1.2266 - sum_metrics: 1.4539 - val_loss: 0.9446 - val_custom_accuracy: 0.7545 - val_distance: 1.3462 - val_sum_metrics: 1.5918\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 151s - loss: 1.2045 - custom_accuracy: 0.6619 - distance: 1.5512 - sum_metrics: 1.8894 - val_loss: 1.1035 - val_custom_accuracy: 0.7252 - val_distance: 1.5318 - val_sum_metrics: 1.8066\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 1.0235 - custom_accuracy: 0.7409 - distance: 1.3584 - sum_metrics: 1.6175 - val_loss: 1.0282 - val_custom_accuracy: 0.7426 - val_distance: 1.3236 - val_sum_metrics: 1.5810\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.9642 - custom_accuracy: 0.7550 - distance: 1.2468 - sum_metrics: 1.4918 - val_loss: 0.9774 - val_custom_accuracy: 0.7609 - val_distance: 1.2762 - val_sum_metrics: 1.5153\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.9219 - custom_accuracy: 0.7632 - distance: 1.1924 - sum_metrics: 1.4291 - val_loss: 0.9900 - val_custom_accuracy: 0.7490 - val_distance: 1.3272 - val_sum_metrics: 1.5782\n",
      "Epoch 5/5\n",
      "8000/8000 - 143s - loss: 0.8954 - custom_accuracy: 0.7685 - distance: 1.1614 - sum_metrics: 1.3929 - val_loss: 1.0019 - val_custom_accuracy: 0.7485 - val_distance: 1.3675 - val_sum_metrics: 1.6189\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 144s - loss: 1.2140 - custom_accuracy: 0.7966 - distance: 1.6409 - sum_metrics: 1.8443 - val_loss: 1.0951 - val_custom_accuracy: 0.8676 - val_distance: 1.5599 - val_sum_metrics: 1.6923\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 1.0391 - custom_accuracy: 0.8814 - distance: nan - sum_metrics: nan - val_loss: 1.0049 - val_custom_accuracy: 0.8829 - val_distance: nan - val_sum_metrics: nan\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.9964 - custom_accuracy: 0.9007 - distance: nan - sum_metrics: nan - val_loss: 0.9981 - val_custom_accuracy: 0.8894 - val_distance: nan - val_sum_metrics: nan\n",
      "Epoch 4/5\n",
      "8000/8000 - 141s - loss: 0.9464 - custom_accuracy: 0.9226 - distance: nan - sum_metrics: nan - val_loss: 1.0093 - val_custom_accuracy: 0.8879 - val_distance: nan - val_sum_metrics: nan\n",
      "Epoch 5/5\n",
      "8000/8000 - 141s - loss: 0.9110 - custom_accuracy: 0.9390 - distance: nan - sum_metrics: nan - val_loss: 1.0113 - val_custom_accuracy: 0.8934 - val_distance: nan - val_sum_metrics: nan\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 144s - loss: 1.2514 - custom_accuracy: 0.5527 - distance: 1.5310 - sum_metrics: 1.9782 - val_loss: 1.1566 - val_custom_accuracy: 0.5670 - val_distance: 1.2919 - val_sum_metrics: 1.7250\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 1.0912 - custom_accuracy: 0.5724 - distance: 1.2260 - sum_metrics: 1.6536 - val_loss: 1.0448 - val_custom_accuracy: 0.5580 - val_distance: 1.2064 - val_sum_metrics: 1.6484\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.9981 - custom_accuracy: 0.6034 - distance: 1.1161 - sum_metrics: 1.5127 - val_loss: 1.0216 - val_custom_accuracy: 0.6275 - val_distance: 1.1936 - val_sum_metrics: 1.5661\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.9618 - custom_accuracy: 0.6181 - distance: 1.0783 - sum_metrics: 1.4602 - val_loss: 1.0138 - val_custom_accuracy: 0.6706 - val_distance: 1.1906 - val_sum_metrics: 1.5200\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.9185 - custom_accuracy: 0.6357 - distance: 1.0360 - sum_metrics: 1.4002 - val_loss: 1.0673 - val_custom_accuracy: 0.5387 - val_distance: 1.1950 - val_sum_metrics: 1.6563\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 143s - loss: 1.2646 - custom_accuracy: 0.5685 - distance: 1.9843 - sum_metrics: 2.4158 - val_loss: 1.1009 - val_custom_accuracy: 0.6652 - val_distance: 1.6748 - val_sum_metrics: 2.0096\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 1.0691 - custom_accuracy: 0.6654 - distance: 1.4542 - sum_metrics: 1.7889 - val_loss: 1.0445 - val_custom_accuracy: 0.6622 - val_distance: 1.4520 - val_sum_metrics: 1.7898\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 1.0201 - custom_accuracy: 0.6867 - distance: 1.3477 - sum_metrics: 1.6609 - val_loss: 1.0516 - val_custom_accuracy: 0.6414 - val_distance: 1.3678 - val_sum_metrics: 1.7265\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.9791 - custom_accuracy: 0.7135 - distance: 1.2454 - sum_metrics: 1.5319 - val_loss: 1.0426 - val_custom_accuracy: 0.6850 - val_distance: 1.3651 - val_sum_metrics: 1.6800\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.9673 - custom_accuracy: 0.7303 - distance: 1.2115 - sum_metrics: 1.4813 - val_loss: 1.0367 - val_custom_accuracy: 0.7044 - val_distance: 1.4553 - val_sum_metrics: 1.7509\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 150s - loss: 1.2169 - custom_accuracy: 0.6242 - distance: 2.1500 - sum_metrics: 2.5258 - val_loss: 1.1190 - val_custom_accuracy: 0.6910 - val_distance: 2.0610 - val_sum_metrics: 2.3700\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 1.0225 - custom_accuracy: 0.7107 - distance: 1.6056 - sum_metrics: 1.8949 - val_loss: 0.9796 - val_custom_accuracy: 0.7212 - val_distance: 1.4597 - val_sum_metrics: 1.7385\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.9381 - custom_accuracy: 0.7481 - distance: 1.3171 - sum_metrics: 1.5690 - val_loss: 0.9762 - val_custom_accuracy: 0.7277 - val_distance: 1.4638 - val_sum_metrics: 1.7361\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.8821 - custom_accuracy: 0.7814 - distance: 1.2098 - sum_metrics: 1.4284 - val_loss: 0.9688 - val_custom_accuracy: 0.7292 - val_distance: 1.4273 - val_sum_metrics: 1.6981\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.8458 - custom_accuracy: 0.8062 - distance: nan - sum_metrics: nan - val_loss: 0.9768 - val_custom_accuracy: 0.7133 - val_distance: 1.3814 - val_sum_metrics: 1.6681\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 143s - loss: 1.2013 - custom_accuracy: 0.6327 - distance: 2.2098 - sum_metrics: 2.5771 - val_loss: 1.0316 - val_custom_accuracy: 0.7083 - val_distance: 1.9989 - val_sum_metrics: 2.2906\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 0.9976 - custom_accuracy: 0.7060 - distance: 1.7251 - sum_metrics: 2.0191 - val_loss: 0.9506 - val_custom_accuracy: 0.7312 - val_distance: 1.6563 - val_sum_metrics: 1.9251\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.9513 - custom_accuracy: 0.7222 - distance: 1.5919 - sum_metrics: 1.8696 - val_loss: 0.9254 - val_custom_accuracy: 0.7222 - val_distance: 1.5745 - val_sum_metrics: 1.8523\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 - 143s - loss: 0.8471 - custom_accuracy: 0.7740 - distance: 1.3485 - sum_metrics: 1.5745 - val_loss: 0.9091 - val_custom_accuracy: 0.7247 - val_distance: 1.4466 - val_sum_metrics: 1.7219\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.7991 - custom_accuracy: 0.7977 - distance: 1.2451 - sum_metrics: 1.4473 - val_loss: 0.9476 - val_custom_accuracy: 0.7366 - val_distance: 1.5858 - val_sum_metrics: 1.8492\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 143s - loss: 1.1239 - custom_accuracy: 0.6304 - distance: 2.2375 - sum_metrics: 2.6072 - val_loss: 1.1359 - val_custom_accuracy: 0.6037 - val_distance: 2.4685 - val_sum_metrics: 2.8649\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 0.9178 - custom_accuracy: 0.7046 - distance: 1.7479 - sum_metrics: 2.0433 - val_loss: 0.8387 - val_custom_accuracy: 0.7262 - val_distance: 1.5473 - val_sum_metrics: 1.8212\n",
      "Epoch 3/5\n",
      "8000/8000 - 141s - loss: 0.7869 - custom_accuracy: 0.7545 - distance: 1.4651 - sum_metrics: 1.7106 - val_loss: 0.8339 - val_custom_accuracy: 0.7366 - val_distance: 1.6154 - val_sum_metrics: 1.8788\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.7318 - custom_accuracy: 0.7834 - distance: 1.3740 - sum_metrics: 1.5906 - val_loss: 0.8227 - val_custom_accuracy: 0.7411 - val_distance: 1.5341 - val_sum_metrics: 1.7930\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.6517 - custom_accuracy: 0.8223 - distance: 1.2480 - sum_metrics: 1.4257 - val_loss: 0.8445 - val_custom_accuracy: 0.7267 - val_distance: 1.4624 - val_sum_metrics: 1.7357\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 144s - loss: 1.0295 - custom_accuracy: 0.6283 - distance: 2.2656 - sum_metrics: 2.6374 - val_loss: 0.7493 - val_custom_accuracy: 0.7277 - val_distance: 1.8238 - val_sum_metrics: 2.0961\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 0.7643 - custom_accuracy: 0.7143 - distance: 1.7606 - sum_metrics: 2.0464 - val_loss: 0.7089 - val_custom_accuracy: 0.7321 - val_distance: 1.6363 - val_sum_metrics: 1.9041\n",
      "Epoch 3/5\n"
     ]
    }
   ],
   "source": [
    "best_acc = (0, 0)\n",
    "best_dist = (5, 0)\n",
    "best_total = (6, 0)\n",
    "mid_vals = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1]\n",
    "for i in mid_vals:\n",
    "    model, history = models.baseline_with_custom_metrics(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                     models.stars_to_probs(training_stars, i), \n",
    "                                                     models.stars_to_probs(test_stars, i))\n",
    "    if history.history['val_custom_accuracy'][4] > best_acc[0]:\n",
    "        best_acc = (history.history['val_custom_accuracy'][4], i)\n",
    "    if history.history['val_distance'][4] < best_dist[0]:\n",
    "        best_dist = (history.history['val_distance'][4], i)\n",
    "    if history.history['val_sum_metrics'][4] < best_total[0]:\n",
    "        best_total = (history.history['val_sum_metrics'][4], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 150s - loss: 1.0783 - accuracy: 0.6030 - custom_accuracy: 0.6030 - distance: 2.1317 - sum_metrics: 2.5287 - val_loss: 0.8301 - val_accuracy: 0.6940 - val_custom_accuracy: 0.6885 - val_distance: 1.8891 - val_sum_metrics: 2.2006\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 0.8192 - accuracy: 0.6861 - custom_accuracy: 0.6861 - distance: 1.6697 - sum_metrics: 1.9836 - val_loss: 0.7880 - val_accuracy: 0.7110 - val_custom_accuracy: 0.7054 - val_distance: 1.7022 - val_sum_metrics: 1.9969\n",
      "Epoch 3/5\n",
      "8000/8000 - 143s - loss: 0.7850 - accuracy: 0.7023 - custom_accuracy: 0.7023 - distance: 1.6790 - sum_metrics: 1.9767 - val_loss: 0.8482 - val_accuracy: 0.6875 - val_custom_accuracy: 0.6820 - val_distance: 1.8604 - val_sum_metrics: 2.1783\n",
      "Epoch 4/5\n",
      "8000/8000 - 146s - loss: 0.6601 - accuracy: 0.7499 - custom_accuracy: 0.7499 - distance: 1.4623 - sum_metrics: 1.7125 - val_loss: 0.7491 - val_accuracy: 0.7220 - val_custom_accuracy: 0.7163 - val_distance: 1.6018 - val_sum_metrics: 1.8855\n",
      "Epoch 5/5\n",
      "8000/8000 - 149s - loss: 0.5241 - accuracy: 0.8031 - custom_accuracy: 0.8031 - distance: 1.2910 - sum_metrics: 1.4879 - val_loss: 0.8239 - val_accuracy: 0.6795 - val_custom_accuracy: 0.6741 - val_distance: 1.5655 - val_sum_metrics: 1.8913\n"
     ]
    }
   ],
   "source": [
    "model3, history = models.baseline_with_custom_metrics(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                     training_stars, \n",
    "                                                     test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test_df[\"text\"][:10000]), maxlen = 250, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model3.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stars = np.array([np.array([0, 1]) if np.argmax(i) != 0 else np.array([1, 0]) for i in test_stars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-88395b7e2e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_most_confused\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_stars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "models.get_most_confused(temp, test_stars, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106716"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "temper = test_df[[\"stars_1.0\", \"stars_2.0\", \"stars_3.0\", \"stars_4.0\", \"stars_5.0\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7182"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = 0\n",
    "for i in range(10000):\n",
    "    if np.argmax(temp[i]) == np.argmax(temper[i]):\n",
    "        corr += 1\n",
    "corr / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 144s - loss: 0.4217 - accuracy: 0.8065 - custom_accuracy: 0.8065 - val_loss: 0.3518 - val_accuracy: 0.8540 - val_custom_accuracy: 0.8472\n",
      "Epoch 2/5\n",
      "8000/8000 - 141s - loss: 0.2471 - accuracy: 0.9070 - custom_accuracy: 0.9070 - val_loss: 0.2699 - val_accuracy: 0.8885 - val_custom_accuracy: 0.8814\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.1883 - accuracy: 0.9308 - custom_accuracy: 0.9308 - val_loss: 0.2763 - val_accuracy: 0.9050 - val_custom_accuracy: 0.8978\n",
      "Epoch 4/5\n",
      "8000/8000 - 145s - loss: 0.1346 - accuracy: 0.9540 - custom_accuracy: 0.9540 - val_loss: 0.2819 - val_accuracy: 0.9025 - val_custom_accuracy: 0.8953\n",
      "Epoch 5/5\n",
      "8000/8000 - 143s - loss: 0.0971 - accuracy: 0.9704 - custom_accuracy: 0.9704 - val_loss: 0.3435 - val_accuracy: 0.8820 - val_custom_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "model, history = models.tree_node(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                     np.array([np.array([1, 0]) if np.argmax(i) < 3 else np.array([0, 1]) for i in training_stars]), \n",
    "                                                     np.array([np.array([1, 0]) if np.argmax(i) < 3 else np.array([0, 1]) for i in test_stars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 148s - loss: 0.3457 - accuracy: 0.8524 - custom_accuracy: 0.8524 - val_loss: 0.2372 - val_accuracy: 0.9080 - val_custom_accuracy: 0.9008\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 0.2383 - accuracy: 0.9016 - custom_accuracy: 0.9016 - val_loss: 0.2230 - val_accuracy: 0.9120 - val_custom_accuracy: 0.9048\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.1578 - accuracy: 0.9390 - custom_accuracy: 0.9390 - val_loss: 0.3111 - val_accuracy: 0.8810 - val_custom_accuracy: 0.8740\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.1125 - accuracy: 0.9582 - custom_accuracy: 0.9582 - val_loss: 0.3397 - val_accuracy: 0.9020 - val_custom_accuracy: 0.8948\n",
      "Epoch 5/5\n",
      "8000/8000 - 143s - loss: 0.0880 - accuracy: 0.9682 - custom_accuracy: 0.9682 - val_loss: 0.2213 - val_accuracy: 0.9180 - val_custom_accuracy: 0.9107\n"
     ]
    }
   ],
   "source": [
    "model2, history2 = models.tree_node(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                     np.array([np.array([0, 1]) if np.argmax(i) != 0 else np.array([1, 0]) for i in training_stars]), \n",
    "                                                     np.array([np.array([0, 1]) if np.argmax(i) != 0 else np.array([1, 0]) for i in test_stars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Below LOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 5000, oov_token = '<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reviews, training_stars, test_reviews, test_stars = models.get_fit_samples(train_df, test_df, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test_reviews), maxlen = 250, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 150s - loss: 1.0162 - accuracy: 0.6329 - custom_accuracy: 0.6329 - distance: 2.3984 - sum_metrics: 2.7655 - val_loss: 0.9211 - val_accuracy: 0.6695 - val_custom_accuracy: 0.6642 - val_distance: 1.8132 - val_sum_metrics: 2.1490\n",
      "Epoch 2/5\n",
      "8000/8000 - 149s - loss: 0.7447 - accuracy: 0.7337 - custom_accuracy: 0.7337 - distance: 1.7908 - sum_metrics: 2.0571 - val_loss: 0.8381 - val_accuracy: 0.6835 - val_custom_accuracy: 0.6781 - val_distance: 1.5956 - val_sum_metrics: 1.9175\n",
      "Epoch 3/5\n",
      "8000/8000 - 151s - loss: 0.6172 - accuracy: 0.7731 - custom_accuracy: 0.7731 - distance: 1.5341 - sum_metrics: 1.7609 - val_loss: 0.8454 - val_accuracy: 0.6765 - val_custom_accuracy: 0.6711 - val_distance: 1.6210 - val_sum_metrics: 1.9499\n",
      "Epoch 4/5\n",
      "8000/8000 - 160s - loss: 0.5282 - accuracy: 0.8016 - custom_accuracy: 0.8016 - distance: 1.4196 - sum_metrics: 1.6180 - val_loss: 0.7865 - val_accuracy: 0.6985 - val_custom_accuracy: 0.6930 - val_distance: 1.4434 - val_sum_metrics: 1.7504\n",
      "Epoch 5/5\n",
      "8000/8000 - 158s - loss: 0.4270 - accuracy: 0.8422 - custom_accuracy: 0.8422 - distance: 1.2727 - sum_metrics: 1.4305 - val_loss: 0.9234 - val_accuracy: 0.6690 - val_custom_accuracy: 0.6637 - val_distance: 1.5589 - val_sum_metrics: 1.8952\n"
     ]
    }
   ],
   "source": [
    "bwcm_no_probs, bwcm_no_probs_hist = models.baseline_with_custom_metrics(5, tokenizer, training_reviews, test_reviews, training_stars, test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 143s - loss: 1.2666 - accuracy: 0.6264 - custom_accuracy: 0.6264 - distance: 2.5170 - sum_metrics: 2.8906 - val_loss: 0.8605 - val_accuracy: 0.7330 - val_custom_accuracy: 0.7272 - val_distance: 2.0223 - val_sum_metrics: 2.2951\n",
      "Epoch 2/5\n",
      "8000/8000 - 140s - loss: 1.0465 - accuracy: 0.7281 - custom_accuracy: 0.7281 - distance: 1.8309 - sum_metrics: 2.1027 - val_loss: 0.8015 - val_accuracy: 0.7495 - val_custom_accuracy: 0.7436 - val_distance: 1.8526 - val_sum_metrics: 2.1091\n",
      "Epoch 3/5\n",
      "8000/8000 - 141s - loss: 0.9599 - accuracy: 0.7714 - custom_accuracy: 0.7714 - distance: 1.5442 - sum_metrics: 1.7728 - val_loss: 0.7818 - val_accuracy: 0.7520 - val_custom_accuracy: 0.7460 - val_distance: 1.7461 - val_sum_metrics: 2.0000\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.9099 - accuracy: 0.8012 - custom_accuracy: 0.8012 - distance: 1.4307 - sum_metrics: 1.6294 - val_loss: 0.7562 - val_accuracy: 0.7625 - val_custom_accuracy: 0.7564 - val_distance: 1.7182 - val_sum_metrics: 1.9618\n",
      "Epoch 5/5\n",
      "8000/8000 - 141s - loss: 0.8597 - accuracy: 0.8336 - custom_accuracy: 0.8336 - distance: 1.2834 - sum_metrics: 1.4498 - val_loss: 0.8255 - val_accuracy: 0.7165 - val_custom_accuracy: 0.7108 - val_distance: 1.4994 - val_sum_metrics: 1.7886\n"
     ]
    }
   ],
   "source": [
    "bwcm_8_probs, bwcm_8_probs_hist = models.baseline_with_custom_metrics(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      models.stars_to_probs(training_stars, .8), \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reviews_4_5, training_stars_4_5, test_reviews_4_5, test_stars_4_5 = models.get_binaries_sets(train_df, \n",
    "                                                                                                      test_df, 10000, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1998 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 143s - loss: 0.6013 - accuracy: 0.6758 - custom_accuracy: 0.6758 - val_loss: 0.5528 - val_accuracy: 0.7312 - val_custom_accuracy: 0.7247\n",
      "Epoch 2/5\n",
      "8000/8000 - 140s - loss: 0.4993 - accuracy: 0.7719 - custom_accuracy: 0.7719 - val_loss: 0.5197 - val_accuracy: 0.7487 - val_custom_accuracy: 0.7421\n",
      "Epoch 3/5\n",
      "8000/8000 - 141s - loss: 0.4333 - accuracy: 0.8084 - custom_accuracy: 0.8084 - val_loss: 0.5147 - val_accuracy: 0.7523 - val_custom_accuracy: 0.7455\n",
      "Epoch 4/5\n",
      "8000/8000 - 143s - loss: 0.3674 - accuracy: 0.8469 - custom_accuracy: 0.8469 - val_loss: 0.5503 - val_accuracy: 0.7497 - val_custom_accuracy: 0.7431\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.3005 - accuracy: 0.8827 - custom_accuracy: 0.8827 - val_loss: 0.6000 - val_accuracy: 0.7307 - val_custom_accuracy: 0.7242\n"
     ]
    }
   ],
   "source": [
    "tn_4_5_differ, tn_4_5_differ_hist = models.tree_node(5, tokenizer, training_reviews_4_5, test_reviews_4_5, \n",
    "                                                     np.array([np.array([1, 0]) if np.argmax(i) == 3 else np.array([0, 1]) for i in training_stars_4_5]), \n",
    "                                                     np.array([np.array([1, 0]) if np.argmax(i) == 3 else np.array([0, 1]) for i in test_stars_4_5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m_t_f = models.model_to_func(bwcm_8_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stars = test_m_t_f(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7165"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = 0\n",
    "for i in range(2000):\n",
    "    if np.argmax(pred_stars[i]) == np.argmax(test_stars[i]):\n",
    "        corr += 1\n",
    "corr / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0-1': (57, 0.4439952734269594),\n",
       "  '0-2': (28, 0.38352746835776735),\n",
       "  '0-3': (13, 0.49363646140465367),\n",
       "  '0-4': (23, 0.49136230219965393),\n",
       "  '1-0': (42, 0.5921389730203719),\n",
       "  '1-2': (14, 0.39672087345804485),\n",
       "  '1-3': (14, 0.4847007840871811),\n",
       "  '1-4': (7, 0.589742124080658),\n",
       "  '2-0': (20, 0.5807071596384048),\n",
       "  '2-1': (14, 0.413681223988533),\n",
       "  '2-3': (28, 0.5052007660269737),\n",
       "  '2-4': (12, 0.5403221646944681),\n",
       "  '3-0': (7, 0.4590423447745187),\n",
       "  '3-1': (9, 0.40263227621714276),\n",
       "  '3-2': (17, 0.37233542870072756),\n",
       "  '3-4': (77, 0.5810773875806239),\n",
       "  '4-0': (20, 0.5127536982297898),\n",
       "  '4-1': (3, 0.3949244022369385),\n",
       "  '4-2': (27, 0.3468763121852168),\n",
       "  '4-3': (135, 0.5038947580037294)},\n",
       " {'0': 558, '1': 105, '2': 89, '3': 210, '4': 1038})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.get_most_confused(pred_stars, test_stars, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.1': 0.843469591226321,\n",
       " '0.2': 0.843469591226321,\n",
       " '0.3': 0.8433734939759037,\n",
       " '0.4': 0.8467824310520939,\n",
       " '0.5': 0.8632855567805954,\n",
       " '0.6': 0.8903225806451613,\n",
       " '0.7': 0.9368029739776952,\n",
       " '0.8': 1.0,\n",
       " '0.9': 0}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.get_probs_when(pred_stars, test_stars, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h_b_t_f = models.helper_binary_to_func(tn_4_5_differ, np.array([0, 0, 0, 1, 0]), np.array([0, 0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_4_5 = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test_reviews_4_5), maxlen = 250, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stars_4_5 = test_h_b_t_f(test_padded_4_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7307307307307307"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = 0\n",
    "for i in range(1998):\n",
    "    if np.argmax(pred_stars_4_5[i]) == np.argmax(test_stars_4_5[i]):\n",
    "        corr += 1\n",
    "corr / 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = models.stack_helper(test_m_t_f, test_h_b_t_f, 3, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_8_probs.save(\"bwcm_8_probs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_4_5_differ.save(\"tn_4_5_differ.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred_stars = tester(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.731"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = 0\n",
    "for i in range(2000):\n",
    "    if np.argmax(double_pred[i]) == np.argmax(test_stars[i]):\n",
    "        corr += 1\n",
    "corr / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0-1': (57, 0.4439952734269594),\n",
       "  '0-2': (28, 0.38352746835776735),\n",
       "  '0-3': (6, 0.5657153824965159),\n",
       "  '0-4': (30, 0.6100444316864013),\n",
       "  '1-0': (42,\n",
       "   <tf.Tensor: id=563616, shape=(), dtype=float32, numpy=0.5293044>),\n",
       "  '1-2': (14, 0.39672087345804485),\n",
       "  '1-3': (7, 0.5750144975525993),\n",
       "  '1-4': (14, 0.794871062040329),\n",
       "  '2-0': (20,\n",
       "   <tf.Tensor: id=563618, shape=(), dtype=float32, numpy=0.5392767>),\n",
       "  '2-1': (14, 0.413681223988533),\n",
       "  '2-3': (17, 0.5812327826724333),\n",
       "  '2-4': (23, 0.7601680859275486),\n",
       "  '3-0': (7, <tf.Tensor: id=563620, shape=(), dtype=float32, numpy=0.521627>),\n",
       "  '3-1': (9, 0.40263227621714276),\n",
       "  '3-2': (17, 0.37233542870072756),\n",
       "  '3-4': (111, 0.7093960256189913),\n",
       "  '4-0': (20,\n",
       "   <tf.Tensor: id=563622, shape=(), dtype=float32, numpy=0.52162707>),\n",
       "  '4-1': (3, 0.3949244022369385),\n",
       "  '4-2': (27, 0.3468763121852168),\n",
       "  '4-3': (72, 0.5689579670627912)},\n",
       " {'0': 558, '1': 105, '2': 89, '3': 210, '4': 1038})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.get_most_confused(double_pred, test_stars, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 305s - loss: 1.0079 - accuracy: 0.6436 - val_loss: 0.7868 - val_accuracy: 0.7260\n",
      "Epoch 2/5\n",
      "8000/8000 - 309s - loss: 0.6941 - accuracy: 0.7513 - val_loss: 0.7621 - val_accuracy: 0.7430\n",
      "Epoch 3/5\n",
      "8000/8000 - 316s - loss: 0.6083 - accuracy: 0.7821 - val_loss: 0.8025 - val_accuracy: 0.7445\n",
      "Epoch 4/5\n",
      "8000/8000 - 313s - loss: 0.4973 - accuracy: 0.8169 - val_loss: 0.8440 - val_accuracy: 0.7140\n",
      "Epoch 5/5\n",
      "8000/8000 - 312s - loss: 0.4207 - accuracy: 0.8484 - val_loss: 1.0018 - val_accuracy: 0.6790\n"
     ]
    }
   ],
   "source": [
    "b, b_hist = models.baseline(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      training_stars, \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.save(\"b.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reviews_1_5, training_stars_1_5, test_reviews_1_5, test_stars_1_5 = models.get_binaries_sets(train_df, \n",
    "                                                                                                      test_df, 10000, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1998 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 145s - loss: 0.6018 - accuracy: 0.6747 - custom_accuracy: 0.6747 - val_loss: 0.5267 - val_accuracy: 0.7513 - val_custom_accuracy: 0.7445\n",
      "Epoch 2/5\n",
      "8000/8000 - 147s - loss: 0.5035 - accuracy: 0.7648 - custom_accuracy: 0.7648 - val_loss: 0.4981 - val_accuracy: 0.7643 - val_custom_accuracy: 0.7574\n",
      "Epoch 3/5\n",
      "8000/8000 - 152s - loss: 0.4317 - accuracy: 0.8095 - custom_accuracy: 0.8095 - val_loss: 0.5033 - val_accuracy: 0.7638 - val_custom_accuracy: 0.7569\n",
      "Epoch 4/5\n",
      "8000/8000 - 149s - loss: 0.3621 - accuracy: 0.8479 - custom_accuracy: 0.8479 - val_loss: 0.5724 - val_accuracy: 0.7397 - val_custom_accuracy: 0.7331\n",
      "Epoch 5/5\n",
      "8000/8000 - 154s - loss: 0.2863 - accuracy: 0.8865 - custom_accuracy: 0.8865 - val_loss: 0.6051 - val_accuracy: 0.7402 - val_custom_accuracy: 0.7336\n"
     ]
    }
   ],
   "source": [
    "tn_1_5_differ, tn_1_5_differ_hist = models.tree_node(5, tokenizer, training_reviews_1_5, test_reviews_1_5, \n",
    "                                                     np.array([np.array([1, 0]) if np.argmax(i) == 3 else np.array([0, 1]) for i in training_stars_1_5]), \n",
    "                                                     np.array([np.array([1, 0]) if np.argmax(i) == 3 else np.array([0, 1]) for i in test_stars_1_5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_1_5_differ.save(\"tn_1_5_differ.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_helper = models.stack_helper(tester, tn_1_5_differ, 0, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_pred = double_helper(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_clean_df = pd.read_csv(\"ultra_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_u = ultra_clean_df.iloc[:int(len(ultra_clean_df) * .8)]\n",
    "test_df_u = ultra_clean_df.iloc[int(len(ultra_clean_df) * .8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reviews_u, training_stars_u, test_reviews_u, test_stars_u = models.get_fit_samples(train_df_u, test_df_u, 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/5\n",
      "5600/5600 - 107s - loss: 1.3012 - accuracy: 0.5600 - custom_accuracy: 0.5600 - distance: 2.1045 - sum_metrics: 2.5445 - val_loss: 0.9780 - val_accuracy: 0.6757 - val_custom_accuracy: 0.6719 - val_distance: 1.9568 - val_sum_metrics: 2.2849\n",
      "Epoch 2/5\n",
      "5600/5600 - 102s - loss: 1.0818 - accuracy: 0.6687 - custom_accuracy: 0.6687 - distance: 1.5288 - sum_metrics: 1.8601 - val_loss: 0.8750 - val_accuracy: 0.7014 - val_custom_accuracy: 0.6974 - val_distance: 1.6612 - val_sum_metrics: 1.9638\n",
      "Epoch 3/5\n",
      "5600/5600 - 102s - loss: 1.0242 - accuracy: 0.7120 - custom_accuracy: 0.7120 - distance: 1.4070 - sum_metrics: 1.6951 - val_loss: 0.8855 - val_accuracy: 0.7021 - val_custom_accuracy: 0.6982 - val_distance: 1.6177 - val_sum_metrics: 1.9195\n",
      "Epoch 4/5\n",
      "5600/5600 - 102s - loss: 0.9304 - accuracy: 0.7716 - custom_accuracy: 0.7716 - distance: 1.2725 - sum_metrics: 1.5009 - val_loss: 0.8906 - val_accuracy: 0.6864 - val_custom_accuracy: 0.6825 - val_distance: 1.5302 - val_sum_metrics: 1.8476\n",
      "Epoch 5/5\n",
      "5600/5600 - 102s - loss: 0.8868 - accuracy: 0.8138 - custom_accuracy: 0.8138 - distance: 1.2251 - sum_metrics: 1.4114 - val_loss: 0.8840 - val_accuracy: 0.6821 - val_custom_accuracy: 0.6783 - val_distance: 1.6967 - val_sum_metrics: 2.0184\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 5000, oov_token = '<OOV>')\n",
    "tokenizer2.fit_on_texts(list([str(i) for i in train_df_u[\"text\"]]))\n",
    "print(\"model\")\n",
    "bwcm_8_probs_u, bwcm_8_probs_u_hist = models.baseline_with_custom_metrics(5, tokenizer2, training_reviews_u, test_reviews_u, \n",
    "                                                                      models.stars_to_probs(training_stars_u, .8), \n",
    "                                                                      test_stars_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_8_probs_u.save(\"bwcm_8_probs_u.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 275s - loss: 1.4133 - accuracy: 0.5586 - custom_accuracy: 0.5586 - distance: 2.3822 - sum_metrics: 2.8236 - val_loss: 1.0762 - val_accuracy: 0.6350 - val_custom_accuracy: 0.6300 - val_distance: 1.8285 - val_sum_metrics: 2.1985\n",
      "Epoch 2/5\n",
      "8000/8000 - 262s - loss: 1.2680 - accuracy: 0.6442 - custom_accuracy: 0.6442 - distance: 1.9063 - sum_metrics: 2.2621 - val_loss: 1.0014 - val_accuracy: 0.6550 - val_custom_accuracy: 0.6498 - val_distance: 1.6195 - val_sum_metrics: 1.9697\n",
      "Epoch 3/5\n",
      "8000/8000 - 264s - loss: 1.1858 - accuracy: 0.6930 - custom_accuracy: 0.6930 - distance: 1.6410 - sum_metrics: 1.9480 - val_loss: 0.9519 - val_accuracy: 0.6715 - val_custom_accuracy: 0.6662 - val_distance: 1.6054 - val_sum_metrics: 1.9393\n",
      "Epoch 4/5\n",
      "8000/8000 - 263s - loss: 1.1577 - accuracy: 0.7072 - custom_accuracy: 0.7072 - distance: 1.5863 - sum_metrics: 1.8791 - val_loss: 0.9410 - val_accuracy: 0.6885 - val_custom_accuracy: 0.6830 - val_distance: 1.4943 - val_sum_metrics: 1.8113\n",
      "Epoch 5/5\n",
      "8000/8000 - 264s - loss: 1.1205 - accuracy: 0.7243 - custom_accuracy: 0.7243 - distance: 1.4256 - sum_metrics: 1.7014 - val_loss: 0.9346 - val_accuracy: 0.6945 - val_custom_accuracy: 0.6890 - val_distance: 1.4560 - val_sum_metrics: 1.7670\n"
     ]
    }
   ],
   "source": [
    "bwcm_65_probs_dr, bwcm_65_probs_dr_hist = models.baseline_with_custom_metrics_dropout(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      models.stars_to_probs(training_stars, .65), \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_65_probs_dr.save(\"bwcm_65_probs_dr_ft.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_no_probs_dr, bwcm_no_probs_dr_hist = models.baseline_with_custom_metrics_dropout(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      training_stars, \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_no_probs_dr.save(\"bwcm_no_probs_dr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 145s - loss: 1.1171 - accuracy: 0.5876 - custom_accuracy: 0.5876 - distance: 2.3905 - sum_metrics: 2.8029 - val_loss: 0.9680 - val_accuracy: 0.6400 - val_custom_accuracy: 0.6349 - val_distance: 1.8584 - val_sum_metrics: 2.2235\n",
      "Epoch 2/5\n",
      "8000/8000 - 142s - loss: 0.8216 - accuracy: 0.6944 - custom_accuracy: 0.6944 - distance: 1.8169 - sum_metrics: 2.1226 - val_loss: 0.8561 - val_accuracy: 0.6755 - val_custom_accuracy: 0.6701 - val_distance: 1.6018 - val_sum_metrics: 1.9316\n",
      "Epoch 3/5\n",
      "8000/8000 - 142s - loss: 0.6799 - accuracy: 0.7405 - custom_accuracy: 0.7405 - distance: 1.5128 - sum_metrics: 1.7723 - val_loss: 0.8176 - val_accuracy: 0.6825 - val_custom_accuracy: 0.6771 - val_distance: 1.5046 - val_sum_metrics: 1.8276\n",
      "Epoch 4/5\n",
      "8000/8000 - 142s - loss: 0.5890 - accuracy: 0.7735 - custom_accuracy: 0.7735 - distance: 1.3939 - sum_metrics: 1.6204 - val_loss: 0.8192 - val_accuracy: 0.6930 - val_custom_accuracy: 0.6875 - val_distance: 1.4945 - val_sum_metrics: 1.8070\n",
      "Epoch 5/5\n",
      "8000/8000 - 142s - loss: 0.5032 - accuracy: 0.8102 - custom_accuracy: 0.8102 - distance: nan - sum_metrics: nan - val_loss: 0.8326 - val_accuracy: 0.6885 - val_custom_accuracy: 0.6830 - val_distance: 1.4383 - val_sum_metrics: 1.7552\n"
     ]
    }
   ],
   "source": [
    "bwcm_no_probs, bwcm_no_probs_hist = models.baseline_with_custom_metrics(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      training_stars, \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_no_probs.save(\"bwcm_no_probs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 146s - loss: 1.2587 - accuracy: 0.5782 - custom_accuracy: 0.5782 - distance: 2.5341 - sum_metrics: 2.9558 - val_loss: 1.0583 - val_accuracy: 0.5950 - val_custom_accuracy: 0.5903 - val_distance: 1.8084 - val_sum_metrics: 2.2181\n",
      "Epoch 2/5\n",
      "8000/8000 - 143s - loss: 1.0462 - accuracy: 0.6741 - custom_accuracy: 0.6741 - distance: 2.0253 - sum_metrics: 2.3512 - val_loss: 0.9547 - val_accuracy: 0.6255 - val_custom_accuracy: 0.6205 - val_distance: 1.5649 - val_sum_metrics: 1.9443\n",
      "Epoch 3/5\n",
      "8000/8000 - 144s - loss: 0.9828 - accuracy: 0.6959 - custom_accuracy: 0.6959 - distance: 1.8832 - sum_metrics: 2.1874 - val_loss: 0.9124 - val_accuracy: 0.6345 - val_custom_accuracy: 0.6295 - val_distance: 1.5201 - val_sum_metrics: 1.8907\n",
      "Epoch 4/5\n",
      "8000/8000 - 145s - loss: 0.9028 - accuracy: 0.7304 - custom_accuracy: 0.7304 - distance: 1.6350 - sum_metrics: 1.9046 - val_loss: 0.8621 - val_accuracy: 0.6540 - val_custom_accuracy: 0.6488 - val_distance: 1.3625 - val_sum_metrics: 1.7137\n",
      "Epoch 5/5\n",
      "8000/8000 - 145s - loss: 0.8634 - accuracy: 0.7455 - custom_accuracy: 0.7455 - distance: 1.5715 - sum_metrics: 1.8260 - val_loss: 0.8549 - val_accuracy: 0.6630 - val_custom_accuracy: 0.6577 - val_distance: 1.3774 - val_sum_metrics: 1.7196\n"
     ]
    }
   ],
   "source": [
    "bwcm_9_probs_dr, bwcm_9_probs_dr_hist = models.baseline_with_custom_metrics_dropout(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      models.stars_to_probs(training_stars, .9), \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_9_probs_dr.save(\"bwcm_9_probs_dr_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "ones = [] \n",
    "twos = []\n",
    "threes = []\n",
    "fours = []\n",
    "fives = []\n",
    "for i in range(100000):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if len(ones) == 2000 and len(twos) == 2000 and len(threes) == 2000 and len(fours) == 2000 and len(fives) == 2000:\n",
    "        break\n",
    "    if len(ones) != 2000 and train_df.iloc[i][3] == 1:\n",
    "        ones.append(train_df.iloc[i][2])\n",
    "    if len(twos) != 2000 and train_df.iloc[i][4] == 1:\n",
    "        twos.append(train_df.iloc[i][2])\n",
    "    if len(threes) != 2000 and train_df.iloc[i][5] == 1:\n",
    "        threes.append(train_df.iloc[i][2])\n",
    "    if len(fours) != 2000 and train_df.iloc[i][6] == 1:\n",
    "        fours.append(train_df.iloc[i][2])\n",
    "    if len(fives) != 2000 and train_df.iloc[i][7] == 1:\n",
    "        fives.append(train_df.iloc[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [(i, np.array([1, 0, 0, 0, 0])) for i in ones]\n",
    "twos = [(i, np.array([0, 1, 0, 0, 0])) for i in twos]\n",
    "threes = [(i, np.array([0, 0, 1, 0, 0])) for i in threes]\n",
    "fours = [(i, np.array([0, 0, 0, 1, 0])) for i in fours]\n",
    "fives = [(i, np.array([0, 0, 0, 0, 1])) for i in fives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = ones + twos + threes + fours + fives\n",
    "np.random.shuffle(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 - 186s - loss: 1.4876 - accuracy: 0.3387 - custom_accuracy: 0.3382 - distance: 1.7035 - sum_metrics: 2.3653 - val_loss: 1.5162 - val_accuracy: 0.3690 - val_custom_accuracy: 0.3661 - val_distance: 2.1995 - val_sum_metrics: 2.8334\n",
      "Epoch 2/5\n",
      "10000/10000 - 180s - loss: 1.3444 - accuracy: 0.4427 - custom_accuracy: 0.4420 - distance: 1.5385 - sum_metrics: 2.0965 - val_loss: 0.9760 - val_accuracy: 0.6725 - val_custom_accuracy: 0.6672 - val_distance: 1.4704 - val_sum_metrics: 1.8033\n",
      "Epoch 3/5\n",
      "10000/10000 - 183s - loss: 1.1990 - accuracy: 0.5159 - custom_accuracy: 0.5151 - distance: 1.3486 - sum_metrics: 1.8335 - val_loss: 0.9373 - val_accuracy: 0.6770 - val_custom_accuracy: 0.6716 - val_distance: 1.5496 - val_sum_metrics: 1.8780\n",
      "Epoch 4/5\n",
      "10000/10000 - 183s - loss: 1.1261 - accuracy: 0.5648 - custom_accuracy: 0.5639 - distance: 1.2910 - sum_metrics: 1.7271 - val_loss: 0.8613 - val_accuracy: 0.6850 - val_custom_accuracy: 0.6796 - val_distance: 1.3815 - val_sum_metrics: 1.7019\n",
      "Epoch 5/5\n",
      "10000/10000 - 651s - loss: 1.0726 - accuracy: 0.5890 - custom_accuracy: 0.5881 - distance: 1.2358 - sum_metrics: 1.6478 - val_loss: 0.8475 - val_accuracy: 0.6920 - val_custom_accuracy: 0.6865 - val_distance: 1.3792 - val_sum_metrics: 1.6926\n"
     ]
    }
   ],
   "source": [
    "bwcm_9_probs_dr_e, bwcm_9_probs_dr_e_hist = models.baseline_with_custom_metrics_dropout(5, tokenizer, [i[0] for i in training], test_reviews, \n",
    "                                                                      models.stars_to_probs(np.array([i[1] for i in training]), .9), \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_9_probs_dr_e.save(\"bwcm_9_probs_dr_e_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 - 161s - loss: 1.2643 - accuracy: 0.5711 - custom_accuracy: 0.5711 - distance: 2.5711 - sum_metrics: 2.9999 - val_loss: 1.1316 - val_accuracy: 0.6115 - val_custom_accuracy: 0.6066 - val_distance: 2.6524 - val_sum_metrics: 3.0457\n",
      "Epoch 2/5\n",
      "8000/8000 - 144s - loss: 1.0712 - accuracy: 0.6636 - custom_accuracy: 0.6636 - distance: 2.0983 - sum_metrics: 2.4346 - val_loss: 0.8174 - val_accuracy: 0.7175 - val_custom_accuracy: 0.7118 - val_distance: 1.9970 - val_sum_metrics: 2.2851\n",
      "Epoch 3/5\n",
      "8000/8000 - 145s - loss: 0.9703 - accuracy: 0.7036 - custom_accuracy: 0.7036 - distance: 1.7905 - sum_metrics: 2.0869 - val_loss: 0.7445 - val_accuracy: 0.7440 - val_custom_accuracy: 0.7381 - val_distance: 1.7445 - val_sum_metrics: 2.0064\n",
      "Epoch 4/5\n",
      "8000/8000 - 145s - loss: 0.9143 - accuracy: 0.7261 - custom_accuracy: 0.7261 - distance: 1.6695 - sum_metrics: 1.9434 - val_loss: 0.6916 - val_accuracy: 0.7540 - val_custom_accuracy: 0.7480 - val_distance: 1.5736 - val_sum_metrics: 1.8256\n",
      "Epoch 5/5\n",
      "8000/8000 - 145s - loss: 0.8680 - accuracy: 0.7415 - custom_accuracy: 0.7415 - distance: 1.5655 - sum_metrics: 1.8240 - val_loss: 0.6833 - val_accuracy: 0.7605 - val_custom_accuracy: 0.7545 - val_distance: 1.6076 - val_sum_metrics: 1.8531\n"
     ]
    }
   ],
   "source": [
    "bwcm_9_probs_dr, hist = models.baseline_with_custom_metrics_dropout(5, tokenizer, training_reviews, test_reviews, \n",
    "                                                                      models.stars_to_probs(training_stars, .9), \n",
    "                                                                      test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwcm_9_probs_dr.save(\"bwcm_9_probs_dr_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
